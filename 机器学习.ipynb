{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#import missingno as msno\n",
    "train_data = pd.read_csv(r'D:/jupyterfile/used_car_train_20200313.csv', sep=' ')\n",
    "test_data = pd.read_csv(r'D:/jupyterfile/used_car_testB_20200421.csv', sep=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据探索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#查看数据分布及缺失情况\n",
    "stats = []\n",
    "for col in train_data.columns:\n",
    "    stats.append((col, train_data[col].nunique(), train_data[col].isnull().sum() * 100 / train_data.shape[0],train_data[col].value_counts(normalize=True, dropna=False).values[0] * 100, train_data[col].dtype))\n",
    "stats_df = pd.DataFrame(stats, columns=['Feature', 'Unique_values', 'Percentage of missing values','Percentage of values in the biggest category', 'type'])\n",
    "stats_df.sort_values('Percentage of missing values', ascending=False, inplace=True)\n",
    "stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看object格式的数据字段\n",
    "train_data['notRepairedDamage'].value_counts()\n",
    "test_data['notRepairedDamage'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 空替换'-'\n",
    "train_data['notRepairedDamage'].replace('-', np.nan, inplace=True)\n",
    "test_data['notRepairedDamage'].replace('-', np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#查看seller分布\n",
    "train_data['seller'].value_counts().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#查看offer type分布\n",
    "train_data[\"offerType\"].value_counts().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#删除变量\n",
    "train_data.drop(['seller','offerType'],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#缺失值可视化\n",
    "missing = Train_data.isnull().sum()\n",
    "missing = missing[missing > 0]\n",
    "missing.sort_values(inplace=True)\n",
    "missing.plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#查看价格的分布\n",
    "import scipy.stats as st\n",
    "y = Train_data['price']\n",
    "plt.figure(1); plt.title('Johnson SU')\n",
    "sns.distplot(y, kde=False, fit=st.johnsonsu)\n",
    "plt.figure(2); plt.title('Normal')\n",
    "sns.distplot(y, kde=False, fit=st.norm)\n",
    "plt.figure(3); plt.title('Log Normal')\n",
    "sns.distplot(y, kde=False, fit=st.lognorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#查看价格的skewness and kurtosis\n",
    "print(\"Skewness: %f\" % train_data['price'].skew())\n",
    "print(\"Kurtosis: %f\" % train_data['price'].kurt())\n",
    "#价格的箱线图\n",
    "train_data['price'].plot(kind='box')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(train_data[train_data['price']<=20000]['price']).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#连续变量相关性\n",
    "num_features = ['power', 'kilometer', 'v_0', 'v_1', 'v_2', 'v_3', 'v_4', 'v_5', 'v_6', 'v_7', 'v_8', 'v_9', 'v_10', 'v_11', 'v_12', 'v_13','v_14','price' ]\n",
    "cat_features = ['name', 'model', 'brand', 'bodyType', 'fuelType', 'gearbox', 'notRepairedDamage', 'regionCode']\n",
    "num_feature_price=train_data[num_features]\n",
    "colormap = plt.cm.magma\n",
    "plt.figure(figsize=(16,12))\n",
    "plt.title('Pearson correlation of continuous features', y=1.05, size=15)\n",
    "sns.heatmap(num_feature_price.corr(),linewidths=0.1,vmax=1.0, square=True, \n",
    "            cmap=colormap, linecolor='white', annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#连续变量分布图\n",
    "num_features = ['power', 'kilometer', 'v_0', 'v_1', 'v_2', 'v_3', 'v_4', 'v_5', 'v_6', 'v_7', 'v_8', 'v_9', 'v_10', 'v_11', 'v_12', 'v_13','v_14','price' ]\n",
    "cat_features = ['name', 'model', 'brand', 'bodyType', 'fuelType', 'gearbox', 'notRepairedDamage', 'regionCode']\n",
    "num_feature_price=train_data[num_features]\n",
    "for col in num_features:\n",
    "    print('{:15}'.format(col), \n",
    "          '特征偏度: {:05.2f}'.format(num_feature_price[col].skew()) , \n",
    "          '   ' ,\n",
    "          '特征峰度: {:06.2f}'.format(num_feature_price[col].kurt())  \n",
    "         )\n",
    "f = pd.melt(train_data, value_vars=num_features)\n",
    "g = sns.FacetGrid(f, col=\"variable\",  col_wrap=3, sharex=False, sharey=False)\n",
    "g = g.map(sns.distplot, \"value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[16,4])\n",
    "plt.subplot(1,3,1)\n",
    "train_data[train_data['power']<=600]['power'].plot(kind='box')\n",
    "plt.subplot(1,3,2)\n",
    "train_data[train_data['power']<=600]['power'].hist()\n",
    "plt.subplot(1,3,3)\n",
    "np.log(train_data[train_data['power']<=600]['power']+1).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#观察定性变量\n",
    "# 将空值填充为nan\n",
    "for c in cat_features:\n",
    "    train_data[c] = train_data[c].astype('category')\n",
    "    if train_data[c].isnull().any():\n",
    "        train_data[c] = train_data[c].cat.add_categories(['nan'])\n",
    "        train_data[c] = train_data[c].fillna('nan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分析不同定类变量与价格之间的关系\n",
    "cat_features = ['name', 'model', 'brand', 'bodyType', 'fuelType', 'gearbox', 'notRepairedDamage', 'regionCode']\n",
    "train_data_copy=train_data[train_data['price']<=20000]\n",
    "plt.figure(figsize=[16,10])\n",
    "plt.subplot(2,2,1)\n",
    "ax = sns.boxplot(x=\"bodyType\", y=\"price\", data=train_data_copy)\n",
    "plt.subplot(2,2,2)\n",
    "ax = sns.boxplot(x=\"fuelType\", y=\"price\", data=train_data_copy)\n",
    "plt.subplot(2,2,3)\n",
    "ax = sns.boxplot(x=\"gearbox\", y=\"price\", data=train_data_copy)\n",
    "plt.subplot(2,2,4)\n",
    "ax = sns.boxplot(x=\"notRepairedDamage\", y=\"price\", data=train_data_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将日期变量进行拆分 观察与价格的关系\n",
    "df_train=train_data.loc[:,['regDate','creatDate','price']]\n",
    "#转换日期格式\n",
    "df_train['regDate']=df_train['regDate'].astype(str)\n",
    "df_train['creatDate']=df_train['creatDate'].astype(str)\n",
    "df_train['regyear']=df_train['regDate'].str[0:4]\n",
    "df_train['creatyear']=df_train['creatDate'].str[0:4]\n",
    "df_train['regmonth']=df_train['regDate'].str[4:6]\n",
    "df_train['creatmonth']=df_train['creatDate'].str[4:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['regyear'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_copy=df_train[df_train['price']<=20000]\n",
    "plt.figure(figsize=[16,10])\n",
    "plt.subplot(2,1,1)\n",
    "ax = sns.boxplot(x=\"regyear\", y=\"price\", data=df_train_copy)\n",
    "plt.subplot(2,1,2)\n",
    "ax = sns.boxplot(x=\"regmonth\", y=\"price\", data=df_train_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['creatyear'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['creatmonth'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_copy=df_train[df_train['price']<=20000]\n",
    "plt.figure(figsize=[16,4])\n",
    "plt.subplot(1,2,1)\n",
    "ax = sns.boxplot(x=\"creatyear\", y=\"price\", data=df_train_copy)\n",
    "plt.subplot(1,2,2)\n",
    "ax = sns.boxplot(x=\"creatmonth\", y=\"price\", data=df_train_copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特征工程 建模调参 模型融合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor as XGBR\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#import missingno as msno\n",
    "import sys\n",
    "import importlib\n",
    "importlib.reload(sys)\n",
    "import seaborn as sns\n",
    "#import missingno as msno\n",
    "from operator import itemgetter\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn import linear_model\n",
    "importlib.reload(sys)\n",
    "#import missingno as msno\n",
    "from operator import itemgetter\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "Train_data2 = pd.read_csv(r'C:\\Users\\刘浩宇\\Desktop\\机器学习\\used_car_train_20200313.csv', sep=' ')\n",
    "test_data = pd.read_csv(r'C:\\Users\\刘浩宇\\Desktop\\机器学习\\used_car_testB_20200421.csv', sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_data2['notRepairedDamage'].replace('-', np.nan, inplace=True)\n",
    "test_data['notRepairedDamage'].replace('-', np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##众数填补\n",
    "aim_l =Train_data2.isnull().sum()\n",
    "a_column = aim_l[aim_l>0].index.tolist()\n",
    "for ck in a_column:\n",
    "    Train_data2[ck] = Train_data2[ck].fillna(Train_data2[ck].mode()[0])\n",
    "\n",
    "aim_l = test_data.isnull().sum()\n",
    "a_column = aim_l[aim_l>0].index.tolist()\n",
    "for ck in a_column:\n",
    "    test_data[ck] = test_data[ck].fillna(test_data[ck].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#数据异常值处理\n",
    "Train_data2['power'].replace(0,np.nan,inplace=True)\n",
    "Train_data2.loc[Train_data2['power']>600,'power']=np.nan\n",
    "#训练集\n",
    "train_data_cut=Train_data2.loc[:,['brand','bodyType','fuelType','gearbox','power']]\n",
    "it_imputer=IterativeImputer(max_iter=10, random_state=0)\n",
    "train_data_it_imputed=pd.DataFrame(it_imputer.fit_transform(train_data_cut),columns=train_data_cut.columns)    \n",
    "Train_data2['power']=train_data_it_imputed['power']   \n",
    "# 测试集\n",
    "test_data['power'].replace(0,np.nan,inplace=True)\n",
    "test_data.loc[test_data['power']>600,'power']=np.nan\n",
    "test_data['power']=test_data['power'].fillna(-1)\n",
    "test_data_fillna=test_data[test_data['power'].isin([-1])]\n",
    "#合并\n",
    "train_data_power_info=Train_data2.loc[:,['SaleID','brand','bodyType','fuelType','gearbox','power']]\n",
    "test_data_power_info=test_data.loc[:,['SaleID','brand','bodyType','fuelType','gearbox','power']]\n",
    "power_info=pd.concat([train_data_power_info,test_data_power_info],axis=0)\n",
    "power_info.reset_index(drop=True,inplace=True)\n",
    "power_info_cut=power_info.loc[:,['brand','bodyType','fuelType','gearbox','power']]\n",
    "power_info_cut['power'].replace(-1,np.nan,inplace=True)\n",
    "it_imputer=IterativeImputer(max_iter=10, random_state=0)\n",
    "power_info_it_imputed=pd.DataFrame(it_imputer.fit_transform(power_info_cut),columns=power_info_cut.columns)\n",
    "power_info_it_imputed['SaleID']=power_info['SaleID']\n",
    "power_info_it_imputed.rename(columns={'power':'fixpower'},inplace=True)\n",
    "power_info_it_imputed=power_info_it_imputed.loc[:,['SaleID','fixpower']]\n",
    "test_data=pd.merge(test_data,power_info_it_imputed,on='SaleID',how='left')\n",
    "test_data['power']=test_data['power'].mask(test_data['power']==-1,test_data['fixpower'])    \n",
    "del test_data['fixpower']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=Train_data2 .copy()\n",
    "df_test=test_data.copy()\n",
    "#注册年\n",
    "df_train['regyear']=df_train['regDate'].apply(lambda x : str(x)[:4])\n",
    "df_test['regyear']=df_test['regDate'].apply(lambda x : str(x)[:4])\n",
    "df_train['regmonth']=df_train['regDate'].apply(lambda x : str(x)[4:6])\n",
    "df_test['regmonth']=df_test['regDate'].apply(lambda x : str(x)[4:6])\n",
    "df_train['regday']=df_train['regDate'].apply(lambda x : str(x)[6:8])\n",
    "df_test['regday']=df_test['regDate'].apply(lambda x : str(x)[6:8])\n",
    "df_train['regmonth'].replace('00',np.nan,inplace=True)\n",
    "df_test['regmonth'].replace('00',np.nan,inplace=True)\n",
    "# 注册月份00修正\n",
    "year_month_info=pd.DataFrame()\n",
    "year_month_info=df_train.groupby('regyear')['regmonth'].agg(lambda x: x.value_counts().index[0]).reset_index()\n",
    "year_month_info.rename(columns={'regmonth':'fixregmonth'},inplace=True)\n",
    "df_train['regmonth'].replace(np.nan,'00',inplace=True)\n",
    "df_test['regmonth'].replace(np.nan,'00',inplace=True)\n",
    "df_train=pd.merge(df_train,year_month_info,on='regyear',how='left')\n",
    "df_train['regmonth']=df_train['regmonth'].mask(df_train['regmonth']=='00',df_train['fixregmonth'])\n",
    "del df_train['fixregmonth']  \n",
    "df_test=pd.merge(df_test,year_month_info,on='regyear',how='left')\n",
    "df_test['regmonth']=df_test['regmonth'].mask(df_test['regmonth']=='00',df_test['fixregmonth'])\n",
    "del df_test['fixregmonth']  \n",
    "#使用时长（单位分别为日、月份、年）\n",
    "df_train['fix_regdate']=df_train['regyear']+df_train['regmonth']+df_train['regday']\n",
    "df_test['fix_regdate']=df_test['regyear']+df_test['regmonth']+df_test['regday']\n",
    "df_train['used_day']=(pd.to_datetime(df_train['creatDate'], format='%Y%m%d', errors='coerce') - \n",
    "                            pd.to_datetime(df_train['fix_regdate'], format='%Y%m%d', errors='coerce')).dt.days\n",
    "df_test['used_day']=(pd.to_datetime(df_test['creatDate'], format='%Y%m%d', errors='coerce') - \n",
    "                            pd.to_datetime(df_test['fix_regdate'], format='%Y%m%d', errors='coerce')).dt.days\n",
    "df_train['used_year']=(pd.to_datetime(df_train['creatDate'], format='%Y%m%d').dt.year)-(pd.to_datetime(df_train['fix_regdate'], format='%Y%m%d').dt.year)\n",
    "df_test['used_year']=(pd.to_datetime(df_test['creatDate'], format='%Y%m%d').dt.year)-(pd.to_datetime(df_test['fix_regdate'], format='%Y%m%d').dt.year)\n",
    "df_train['used_month']=((pd.to_datetime(df_train['creatDate'], format='%Y%m%d').dt.year)-(pd.to_datetime(df_train['fix_regdate'], format='%Y%m%d').dt.year))*12+((pd.to_datetime(df_train['creatDate'], format='%Y%m%d').dt.month)-(pd.to_datetime(df_train['fix_regdate'], format='%Y%m%d').dt.month))\n",
    "df_test['used_month']=((pd.to_datetime(df_test['creatDate'], format='%Y%m%d').dt.year)-(pd.to_datetime(df_test['fix_regdate'], format='%Y%m%d').dt.year))*12+((pd.to_datetime(df_train['creatDate'], format='%Y%m%d').dt.month)-(pd.to_datetime(df_test['fix_regdate'], format='%Y%m%d').dt.month))\n",
    "df_train.drop(['regDate','regionCode','creatDate','regyear','regmonth','regday','fix_regdate','creatDate'],axis=1,inplace=True)\n",
    "df_test.drop(['regDate','regionCode','creatDate','regyear','regmonth','regday','fix_regdate','creatDate'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "rng=np.random.RandomState(304)\n",
    "q_t=QuantileTransformer(n_quantiles=500,output_distribution='normal',random_state=rng)\n",
    "for i in ['v_1','v_6','v_10']:\n",
    "    X=np.array(df_train[i]);\n",
    "    X=X.reshape(-1,1);\n",
    "    df_train[i]=q_t.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "rng=np.random.RandomState(304)\n",
    "q_t=QuantileTransformer(n_quantiles=500,output_distribution='normal',random_state=rng)\n",
    "for i in ['v_1','v_6','v_10']:\n",
    "    X=np.array(df_test[i]);\n",
    "    X=X.reshape(-1,1);\n",
    "df_test[i]=q_t.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#通过对各指标的统计处理构造新变量\n",
    "df_train2=pd.DataFrame()\n",
    "df_test2=pd.DataFrame()\n",
    "train_gb_brand = df_train.groupby(\"brand\")\n",
    "all_info = {}\n",
    "for kind, kind_data in train_gb_brand:\n",
    "    info = {}\n",
    "    kind_data = kind_data[kind_data['price'] > 0]\n",
    "    info['brand_amount'] = len(kind_data)\n",
    "    info['brand_price_max'] = kind_data.price.max()\n",
    "    info['brand_price_median'] = kind_data.price.median()\n",
    "    info['brand_price_min'] = kind_data.price.min()\n",
    "    info['brand_price_sum'] = kind_data.price.sum()\n",
    "    info['brand_price_std'] = kind_data.price.std()\n",
    "    info['brand_price_average'] = round(kind_data.price.sum() / (len(kind_data) + 1), 2)\n",
    "    all_info[kind] = info\n",
    "brand_fe = pd.DataFrame(all_info).T.reset_index().rename(columns={\"index\": \"brand\"})\n",
    "df_train2 = df_train.merge(brand_fe, how='left', on='brand')\n",
    "df_test2=df_test.merge(brand_fe, how='left', on='brand')\n",
    "#\n",
    "\n",
    "#\n",
    "train_gb_used_month = df_train.groupby(\"used_month\")\n",
    "all_info = {}\n",
    "for kind, kind_data in train_gb_used_month:\n",
    "    info = {}\n",
    "    kind_data = kind_data[kind_data['price'] > 0]\n",
    "    info['used_month_amount'] = len(kind_data)\n",
    "    info['used_month_price_max'] = kind_data.price.max()\n",
    "    info['used_month_price_median'] = kind_data.price.median()\n",
    "    info['used_month_price_min'] = kind_data.price.min()\n",
    "    info['used_month_price_sum'] = kind_data.price.sum()\n",
    "    info['used_month_price_std'] = kind_data.price.std()\n",
    "    info['used_month_price_average'] = round(kind_data.price.sum() / (len(kind_data) + 1), 2)\n",
    "    all_info[kind] = info\n",
    "brand_fe = pd.DataFrame(all_info).T.reset_index().rename(columns={\"index\": \"used_month\"})\n",
    "df_train2 = df_train2.merge(brand_fe, how='left', on='used_month')\n",
    "df_test2=df_test2.merge(brand_fe, how='left', on='used_month')\n",
    "#\n",
    "train_gb_model = df_train.groupby(\"model\")\n",
    "all_info = {}\n",
    "for kind, kind_data in train_gb_model:\n",
    "    info = {}\n",
    "    kind_data = kind_data[kind_data['price'] > 0]\n",
    "    info['model_amount'] = len(kind_data)\n",
    "    info['model_price_max'] = kind_data.price.max()\n",
    "    info['model_price_median'] = kind_data.price.median()\n",
    "    info['model_price_min'] = kind_data.price.min()\n",
    "    info['model_price_sum'] = kind_data.price.sum()\n",
    "    info['model_price_std'] = kind_data.price.std()\n",
    "    info['model_price_average'] = round(kind_data.price.sum() / (len(kind_data) + 1), 2)\n",
    "    all_info[kind] = info\n",
    "brand_fe = pd.DataFrame(all_info).T.reset_index().rename(columns={\"index\": \"model\"})\n",
    "df_train2 = df_train2.merge(brand_fe, how='left', on='model')\n",
    "df_test2=df_test2.merge(brand_fe, how='left', on='model')\n",
    "#\n",
    "train_gb_bodyType = df_train.groupby(\"bodyType\")\n",
    "all_info = {}\n",
    "for kind, kind_data in train_gb_bodyType:\n",
    "    info = {}\n",
    "    kind_data = kind_data[kind_data['price'] > 0]\n",
    "    info['bodyType_amount'] = len(kind_data)\n",
    "    info['bodyType_price_max'] = kind_data.price.max()\n",
    "    info['bodyType_price_median'] = kind_data.price.median()\n",
    "    info['bodyType_price_min'] = kind_data.price.min()\n",
    "    info['bodyType_price_sum'] = kind_data.price.sum()\n",
    "    info['bodyType_price_std'] = kind_data.price.std()\n",
    "    info['bodyType_price_average'] = round(kind_data.price.sum() / (len(kind_data) + 1), 2)\n",
    "    all_info[kind] = info\n",
    "brand_fe = pd.DataFrame(all_info).T.reset_index().rename(columns={\"index\": \"bodyType\"})\n",
    "df_train2 = df_train2.merge(brand_fe, how='left', on='bodyType')\n",
    "df_test2=df_test2.merge(brand_fe, how='left', on='bodyType')\n",
    "#\n",
    "train_gb_fuelType = df_train.groupby(\"fuelType\")\n",
    "all_info = {}\n",
    "for kind, kind_data in train_gb_fuelType:\n",
    "    info = {}\n",
    "    kind_data = kind_data[kind_data['price'] > 0]\n",
    "    info['fuelType_amount'] = len(kind_data)\n",
    "    info['fuelType_price_max'] = kind_data.price.max()\n",
    "    info['fuelType_price_median'] = kind_data.price.median()\n",
    "    info['fuelType_price_min'] = kind_data.price.min()\n",
    "    info['fuelType_price_sum'] = kind_data.price.sum()\n",
    "    info['fuelType_price_std'] = kind_data.price.std()\n",
    "    info['fuelType_price_average'] = round(kind_data.price.sum() / (len(kind_data) + 1), 2)\n",
    "    all_info[kind] = info\n",
    "brand_fe = pd.DataFrame(all_info).T.reset_index().rename(columns={\"index\": \"fuelType\"})\n",
    "df_train2 = df_train2.merge(brand_fe, how='left', on='fuelType')\n",
    "df_test2=df_test2.merge(brand_fe, how='left', on='fuelType')\n",
    "#\n",
    "train_gb_gearbox = df_train.groupby(\"gearbox\")\n",
    "all_info = {}\n",
    "for kind, kind_data in train_gb_gearbox:\n",
    "    info = {}\n",
    "    kind_data = kind_data[kind_data['price'] > 0]\n",
    "    info['gearbox_amount'] = len(kind_data)\n",
    "    info['gearbox_price_max'] = kind_data.price.max()\n",
    "    info['gearbox_price_median'] = kind_data.price.median()\n",
    "    info['gearbox_price_min'] = kind_data.price.min()\n",
    "    info['gearbox_price_sum'] = kind_data.price.sum()\n",
    "    info['gearbox_price_std'] = kind_data.price.std()\n",
    "    info['gearbox_price_average'] = round(kind_data.price.sum() / (len(kind_data) + 1), 2)\n",
    "    all_info[kind] = info\n",
    "brand_fe = pd.DataFrame(all_info).T.reset_index().rename(columns={\"index\": \"gearbox\"})\n",
    "df_train2 = df_train2.merge(brand_fe, how='left', on='gearbox')\n",
    "df_test2=df_test2.merge(brand_fe, how='left', on='gearbox')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#取出连续变量进行后续特征工程处理\n",
    "numeric_features = ['power', 'kilometer','used_day', 'v_0', 'v_1', 'v_2', 'v_3', 'v_4', 'v_5', 'v_6', 'v_7', 'v_8', 'v_9', 'v_10', 'v_11', 'v_12', 'v_13','v_14','price' ]\n",
    "for i in numeric_features:\n",
    "    sta=(df_train2[i]-df_train2[i].mean())/df_train2[i].std()\n",
    "    delete1=df_train2[sta.abs()>3].index\n",
    "    df_train2=df_train2.drop(delete1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train2['train']=1\n",
    "df_test2['train']=0\n",
    "df_data = pd.concat([df_train2, df_test2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#取对数\n",
    "df_data['log_power']=np.log(df_data['power'])\n",
    "df_data['log_price']=np.log(df_data['price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#离散变量的独热编码\n",
    "from sklearn.preprocessing  import OneHotEncoder \n",
    "import pandas as pd\n",
    "classfiy_features=['model','brand','bodyType','fuelType','gearbox','notRepairedDamage']\n",
    "df_data1=df_data[classfiy_features]\n",
    "enc=OneHotEncoder(categories='auto').fit(df_data1)\n",
    "result=enc.transform(df_data1).toarray()\n",
    "newdata=pd.concat([df_data,pd.DataFrame(result)],axis=1)\n",
    "newdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#删掉冗余变量\n",
    "#newdata.drop(['seller','offerType','SaleID', 'name','train','price','power'],axis=1, inplace=True)\n",
    "newdata.drop(classfiy_features,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col=list(newdata.columns)\n",
    "col.insert(0,col.pop(col.index('log_price')))\n",
    "newdata=newdata[col]\n",
    "newdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_1=newdata[0:135481]\n",
    "df_test_1=newdata[135482:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_1.drop(['log_price'],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features1 = ['log_power', 'kilometer','used_day', 'v_0', 'v_1', 'v_2', 'v_3', 'v_4', 'v_5', 'v_6', 'v_7', 'v_8', 'v_9', 'v_10', 'v_11', 'v_12', 'v_13','v_14','log_price' ]\n",
    "data=df_train_1[numeric_features1]\n",
    "x = data.iloc[:,0:-1]\n",
    "y = data.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#自身方差筛选\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "selector=VarianceThreshold()\n",
    "data=selector.fit_transform(data)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f检验\n",
    "from sklearn.feature_selection import f_regression\n",
    "F,pvalues_f=f_regression(x,y)\n",
    "F\n",
    "pvalues_f\n",
    "k=F.shape[0]-(pvalues_f>0.05).sum()\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#互信息法筛选\n",
    "from sklearn.feature_selection import mutual_info_regression as MIC\n",
    "result=MIC(x,y)\n",
    "k=result.shape[0]-sum(result<=0)\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#嵌入法初步筛选\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from xgboost import XGBRegressor as XGBR\n",
    "#data1=df_train_1(frac=0.5,axis=0)\n",
    "x1=df_train_1.iloc[:,1:]\n",
    "y1=df_train_1.iloc[:,0]\n",
    "rfr=XGBR(n_estimators=100)\n",
    "x_embedded=SelectFromModel(rfr,threshold=0.00002).fit_transform(x1,y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#迭代法用嵌入法求出最佳阈值\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from xgboost import XGBRegressor as XGBR\n",
    "data1=df_train_1(frac=0.5,axis=0)\n",
    "from sklearn.model_selection import cross_val_score \n",
    "x1= df_train_1.iloc[:,1:]\n",
    "y1=df_train_1.iloc[:,0]\n",
    "rfr=XGBR(n_estimators=100)\n",
    "x_embedded=SelectFromModel(rfr,threshold=0.00002).fit_transform(x1,y1)\n",
    "rfr.fit(x1,y1).feature_importances_\n",
    "threshold = np.linspace(0,(rfr.fit(x1,y1).feature_importances_).max(),20)\n",
    "score = []\n",
    "for i in threshold:\n",
    "    X_embedded = SelectFromModel(rfr,threshold=i).fit_transform(x1,y1)\n",
    "    once = cross_val_score(rfr,X_embedded,y1,cv=5).mean()\n",
    "    score.append(once)\n",
    "plt.plot(threshold,score)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor as XGBR\n",
    "#data1=df_train_1(frac=0.5,axis=0)\n",
    "x1= df_train_1.iloc[:,1:]\n",
    "y1=df_train_1.iloc[:,0]\n",
    "rfr=XGBR(n_estimators=100)\n",
    "x_embedded=SelectFromModel(rfr,threshold=0.0005).fit_transform(x1,y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_embedded=pd.DataFrame(x_embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=pd.DataFrame([*zip(x1.columns,rfr.fit(x1,y1).feature_importances_)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column = a[a[1]>0.0005][0].values\n",
    "df_test_2=df_test_1[column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#处理数据中字段的格式，通过改变数据格式的方式减小内存，方便后续运算\n",
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() \n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() \n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n",
    "df_train_x= reduce_mem_usage(x_embedded)\n",
    "df_test_x= reduce_mem_usage(df_test_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#用各个模型都跑一下来看一下效果\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error,  make_scorer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from lightgbm.sklearn import LGBMRegressor\n",
    "from xgboost import XGBRegressor as XGBR\n",
    "models = [LinearRegression(),\n",
    "          DecisionTreeRegressor(),\n",
    "          RandomForestRegressor(),\n",
    "          GradientBoostingRegressor(),\n",
    "          LGBMRegressor(n_estimators = 100),\n",
    "          XGBR(n_estimators=100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = dict()\n",
    "for model in models:\n",
    "    model_name = str(model).split('(')[0]\n",
    "    scores = cross_val_score(model, X=df_train_x, y=y1, verbose=0, cv = 5, scoring=make_scorer(mean_absolute_error))\n",
    "    result[model_name] = scores\n",
    "    print(model_name + ' is finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#展示结果\n",
    "result = pd.DataFrame(result)\n",
    "result.index = ['cv' + str(x) for x in range(1, 6)]\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#网格搜索lightgbm\n",
    "from lightgbm.sklearn import LGBMRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "objective = ['regression', 'regression_l1', 'mape', 'huber', 'fair']\n",
    "num_leaves = [3,5,10,15,20,40, 55]\n",
    "max_depth = [3,5,10,15,20,40, 55]\n",
    "n_estimators=np.arrage(0.200,10)\n",
    "parameters = {'objective': objective , 'num_leaves': num_leaves, 'max_depth': max_depth,'n_estimators':n_estimators}\n",
    "model = LGBMRegressor()\n",
    "clf1 = GridSearchCV(model, parameters, cv=5)\n",
    "clf1 = clf1.fit(df_train_x,y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#测试调参后lightgbm效果\n",
    "from lightgbm.sklearn import LGBMRegressor\n",
    "from sklearn.metrics import mean_absolute_error,  make_scorer\n",
    "model = LGBMRegressor(n_estimators=120,\n",
    "                    objective='regression',\n",
    "                           num_leaves=55,\n",
    "                           max_depth=15)\n",
    "np.mean(cross_val_score(model, X=df_train_x, y=y1, verbose=0, cv = 5, scoring=make_scorer(mean_absolute_error)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#贝叶斯调参 lightgbm\n",
    "from bayes_opt import BayesianOptimization\n",
    "def rf_cv(num_leaves, max_depth, subsample, min_child_samples):\n",
    "    val = cross_val_score(\n",
    "        LGBMRegressor(objective = 'regression',\n",
    "            num_leaves=int(num_leaves),\n",
    "            max_depth=int(max_depth),\n",
    "            subsample = subsample,\n",
    "            min_child_samples = int(min_child_samples)\n",
    "        ),\n",
    "        X=df_train_x, y=y1, verbose=0, cv = 5, scoring=make_scorer(mean_absolute_error)\n",
    "    ).mean()\n",
    "    return val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_bo = BayesianOptimization(\n",
    "    rf_cv,\n",
    "    {\n",
    "    'num_leaves': (2, 100),\n",
    "    'max_depth': (2, 100),\n",
    "    'subsample': (0.1, 1),\n",
    "    'min_child_samples' : (2, 100)\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_bo.maximize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#网格搜索xgboost\n",
    "from lightgbm.sklearn import LGBMRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "eta=[0.1 0.2 0.3 0.4 0.5 ]\n",
    "max_depth = [3,5,10,15,20,40, 55]\n",
    "n_estimators=np.arrage(0.200,10)\n",
    "parameters = { 'max_depth': max_depth,'n_estimators':n_estimators,'eta';eta}\n",
    "model =XGBR()\n",
    "clf2 = GridSearchCV(model, parameters, cv=5)\n",
    "clf2 = clf2.fit(df_train_x,y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#用交叉验证测试调参之后xgboost效果\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error,  make_scorer\n",
    "model = XGBR(n_estimators=130,\n",
    "             eta=0.3,\n",
    "             max_depth=15)\n",
    "np.mean(cross_val_score(model, X=df_train_x, y=y1, verbose=0, cv = 5, scoring=make_scorer(mean_absolute_error)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#构建函数方便实施各个方法\n",
    "def build_model_lgb(x_train,y_train):\n",
    "    lgb= LGBMRegressor(n_estimators = 120,max_depth=15, num_leaves=55, objective='regression')\n",
    "    lgb.fit(x_train, y_train)\n",
    "    return lgb\n",
    "def build_model_xgb(x_train,y_train):\n",
    "    xgb = XGBR(n_estimators=130, learning_rate=0.08, eta=0.3, max_depth=15) #, objective ='reg:squarederror'\n",
    "    xgb.fit(x_train, y_train)\n",
    "    return xgb\n",
    "def build_model_xgb1(x_train,y_train):\n",
    "    xgb = XGBR(n_estimators=130, learning_rate=0.08, eta=0.3, max_depth=8) #, objective ='reg:squarederror'\n",
    "    xgb.fit(x_train, y_train)\n",
    "    return xgb\n",
    "def build_model_lr(x_train,y_train):\n",
    "    reg_model = linear_model.LinearRegression()\n",
    "    reg_model.fit(x_train,y_train)\n",
    "    return reg_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#实施过程中由于个人原因前文中把数据导出，现将其导入\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df_train_x= pd.read_csv(r'C:\\Users\\刘浩宇\\Desktop\\df_train_x.csv')\n",
    "df_train_y= pd.read_csv(r'C:\\Users\\刘浩宇\\Desktop\\df_train_y.csv')\n",
    "df_test_x= pd.read_csv(r'C:\\Users\\刘浩宇\\Desktop\\df_test_x.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#划分训练集和测试集\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train_11=df_train_x\n",
    "y_train_11=df_train_y\n",
    "x_train,x_val,y_train,y_val = train_test_split(df_train_x,df_train_y,test_size=0.3)\n",
    "x_test_1=df_test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#删除无用列\n",
    "df_train_y.drop('Unnamed: 0',axis=1,inplace=True)\n",
    "df_train_x.drop('Unnamed: 0',axis=1,inplace=True)\n",
    "df_test_x.drop('Unnamed: 0',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_x.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_x.columns[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#分别用xgboost，lightgbm预测数据\n",
    "print('predict XGB')\n",
    "model_xgb = build_model_xgb(x_train,y_train)\n",
    "val_xgb = model_xgb.predict(x_val)\n",
    "subA_xgb = model_xgb.predict(x_test_1)\n",
    "\n",
    "print('predict lgb')\n",
    "model_lgb = build_model_lgb(x_train,y_train)\n",
    "val_lgb = model_lgb.predict(x_val)\n",
    "subA_lgb = model_lgb.predict(x_test_1)\n",
    "print('Sta inf of lgb:')\n",
    "Sta_inf(subA_lgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sta_inf(data):\n",
    "    print('_min',np.min(data))\n",
    "    print('_max:',np.max(data))\n",
    "    print('_mean',np.mean(data))\n",
    "    print('_ptp',np.ptp(data))\n",
    "    print('_std',np.std(data))\n",
    "    print('_var',np.var(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#模型融合\n",
    "#第一层\n",
    "train_lgb_pred = model_lgb.predict(x_train)\n",
    "train_xgb_pred = model_xgb.predict(x_train)\n",
    "\n",
    "\n",
    "Strak_X_train = pd.DataFrame()\n",
    "Strak_X_train['Method_1'] = train_lgb_pred\n",
    "Strak_X_train['Method_2'] = train_xgb_pred\n",
    "\n",
    "\n",
    "Strak_X_val = pd.DataFrame()\n",
    "Strak_X_val['Method_1'] = val_lgb\n",
    "Strak_X_val['Method_2'] = val_xgb\n",
    "\n",
    "\n",
    "Strak_X_test = pd.DataFrame()\n",
    "Strak_X_test['Method_1'] = subA_lgb\n",
    "Strak_X_test['Method_2'] = subA_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 用线性模型或者xgboost分别进行第二层的融合\n",
    "#model_lr_Stacking = build_model_lr(Strak_X_train,y_train)\n",
    "model_lr_Stacking = build_model_xgb1(Strak_X_train,y_train)\n",
    "## 训练集\n",
    "train_pre_Stacking = model_lr_Stacking.predict(Strak_X_train)\n",
    "print('MAE of Stacking-LR:',mean_absolute_error(y_train,train_pre_Stacking))\n",
    "\n",
    "## 验证集\n",
    "val_pre_Stacking = model_lr_Stacking.predict(Strak_X_val)\n",
    "print('MAE of Stacking-LR:',mean_absolute_error(y_val,val_pre_Stacking))\n",
    "\n",
    "## 预测集\n",
    "print('Predict Stacking-LR...')\n",
    "subA_Stacking = model_lr_Stacking.predict(Strak_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subA_Stacking.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_data2 = pd.read_csv(r'C:\\Users\\刘浩宇\\Desktop\\机器学习\\used_car_sample_submit.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subA_Stacking[subA_Stacking<4.5]=4.5## 去除过小的预测值\n",
    "subA_Stacking_1=np.exp(subA_Stacking)\n",
    "sub = pd.DataFrame()\n",
    "sub['SaleID'] =submit_data2['SaleID']#测试集文件\n",
    "sub['price'] = subA_Stacking_1\n",
    "sub.to_csv(r'C:\\Users\\刘浩宇\\Desktop\\submit_data_3.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Sta inf:')\n",
    "Sta_inf(subA_Stacking)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
